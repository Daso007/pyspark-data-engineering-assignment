{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f7e3f6-5480-4857-9c6e-46f1557c61e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "java.net.NoRouteToHostException: No route to host\n",
       "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
       "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n",
       "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:183)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:130)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:183)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:130)\n",
       "\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:177)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:842)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "java.net.NoRouteToHostException: No route to host\n\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)\n\tat shaded.v9_4.org.eclipse.jetty.io.SelectorManager.doFinishConnect(SelectorManager.java:355)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.processConnect(ManagedSelector.java:347)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector.access$1700(ManagedSelector.java:65)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.processSelected(ManagedSelector.java:676)\n\tat shaded.v9_4.org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:535)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$4(InstrumentedQueuedThreadPool.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:290)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:286)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:130)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.$anonfun$run$3(InstrumentedQueuedThreadPool.scala:183)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:130)\n\tat com.databricks.rpc.ShadedInstrumentedQueuedThreadPool$$anon$2.run(InstrumentedQueuedThreadPool.scala:177)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
       "errorSummary": "Internal error. Attach your notebook to a different compute or restart the current compute.",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import (col, avg, count, lit, current_timestamp, expr, when, sum as spark_sum, greatest, least, coalesce,percentile_approx)\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, LongType,FloatType, DecimalType, TimestampType)\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# --- Configuration - VERIFY ALL THESE PATHS AND CREDENTIALS ---\n",
    "# S3 Paths\n",
    "INPUT_S3_PATH = \"s3a://daso-de-assignment-transaction-chunks/\"\n",
    "OUTPUT_S3_PATH = \"s3a://daso-de-assignment-detection-outputs/\" \n",
    "CUSTOMER_IMPORTANCE_PATH = \"dbfs:/FileStore/CustomerImportance.csv\" # Ensure this file is in DBFS\n",
    "ARCHIVE_S3_PATH = \"s3a://daso-de-assignment-transaction-chunks/archive/\" # Should exist, even if cleanSource is off for now\n",
    "CHECKPOINT_S3_PATH = \"s3a://daso-de-assignment-auxiliary-data/checkpoints/final_run_v5_no_cleansource/\" # <<<< NEW CHECKPOINT PATH\n",
    "\n",
    "# PostgreSQL Connection Details\n",
    "PG_HOST = \"de-assignment-pg-db.c9g4wki6o511.ap-south-1.rds.amazonaws.com\"  \n",
    "PG_PORT = \"5432\"\n",
    "PG_DATABASE = \"transactions_db\"\n",
    "PG_USER = \"postgres\"\n",
    "PG_PASSWORD = \"********\" # <<<< ENSURE THIS IS YOUR CORRECT RDS PASSWORD\n",
    "\n",
    "PG_JDBC_URL = f\"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "PG_PROPERTIES = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MechanismY_Final_Attempt\").getOrCreate()\n",
    "print(\"Spark Session Created.\")\n",
    "\n",
    "# --- Define Schemas ---\n",
    "transaction_schema = StructType([ \n",
    "    StructField(\"step\", IntegerType(), True), StructField(\"customer\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True), StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"zipcodeOri\", StringType(), True), StructField(\"merchant\", StringType(), True),\n",
    "    StructField(\"zipMerchant\", StringType(), True), StructField(\"category\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True), StructField(\"fraud\", IntegerType(), True)\n",
    "])\n",
    "importance_schema = StructType([ \n",
    "    StructField(\"Source\", StringType(), True), StructField(\"Target\", StringType(), True),\n",
    "    StructField(\"Weight\", FloatType(), True), StructField(\"typeTrans\", StringType(), True),\n",
    "    StructField(\"fraud\", IntegerType(), True) # Assumes 'fraud' is the header in CustomerImportance.csv\n",
    "])\n",
    "schema_merchant_summary = StructType([ # For fallback if PG read fails\n",
    "    StructField(\"merchant_id\", StringType(), True), StructField(\"total_transactions\", LongType(), True),\n",
    "    StructField(\"last_updated\", TimestampType(), True)\n",
    "])\n",
    "schema_customer_merchant_summary = StructType([ # For fallback\n",
    "    StructField(\"customer_id\", StringType(), True), StructField(\"merchant_id\", StringType(), True),\n",
    "    StructField(\"transaction_count\", LongType(), True), StructField(\"total_amount_sum\", DecimalType(18, 2), True),\n",
    "    StructField(\"last_updated\", TimestampType(), True)\n",
    "])\n",
    "schema_merchant_gender_summary = StructType([ # For fallback\n",
    "    StructField(\"merchant_id\", StringType(), True), StructField(\"male_transaction_count\", LongType(), True),\n",
    "    StructField(\"female_transaction_count\", LongType(), True), StructField(\"last_updated\", TimestampType(), True)\n",
    "])\n",
    "output_df_schema = StructType([ # For writing detections\n",
    "    StructField(\"YStartTime\", StringType(), True), StructField(\"DetectionTime\", StringType(), True),\n",
    "    StructField(\"PatternId\", StringType(), True), StructField(\"ActionType\", StringType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True), StructField(\"MerchantId\", StringType(), True)\n",
    "])\n",
    "\n",
    "# --- Load CustomerImportance data ---\n",
    "try:\n",
    "    customer_importance_df = spark.read.format(\"csv\").option(\"header\",\"true\").schema(importance_schema).load(CUSTOMER_IMPORTANCE_PATH)\n",
    "    customer_importance_df = customer_importance_df.withColumnRenamed(\"fraud\", \"ci_fraud\")\n",
    "    customer_importance_df.cache()\n",
    "    print(\"CustomerImportance DataFrame loaded and cached.\")\n",
    "    # customer_importance_df.show(5, False)\n",
    "except Exception as e_cust_imp:\n",
    "    print(f\"FATAL ERROR loading CustomerImportance.csv: {e_cust_imp}\")\n",
    "    # Potentially stop execution if this critical data can't be loaded\n",
    "    # For now, we'll let it proceed and it might fail later or produce incorrect results\n",
    "    customer_importance_df = None \n",
    "\n",
    "\n",
    "# --- Pre-calculate 1st percentile weight from CustomerImportance ---\n",
    "weight_percentiles_df = None # Initialize\n",
    "if customer_importance_df is not None:\n",
    "    try:\n",
    "        weight_percentiles_df = customer_importance_df \\\n",
    "            .groupBy(\"Target\", \"typeTrans\") \\\n",
    "            .agg(percentile_approx(\"Weight\", 0.01).alias(\"p1_weight\")) \\\n",
    "            .withColumnRenamed(\"Target\", \"merchant_key\") \\\n",
    "            .withColumnRenamed(\"typeTrans\", \"category_key\")\n",
    "        weight_percentiles_df.cache()\n",
    "        print(\"Calculated 1st percentile weights for CustomerImportance.\")\n",
    "        # weight_percentiles_df.show(5, False) \n",
    "    except Exception as e_perc:\n",
    "        print(f\"Warning: Could not calculate weight percentiles, PatId1 will use fixed fallback: {e_perc}\")\n",
    "        weight_percentiles_df = None\n",
    "else:\n",
    "    print(\"Warning: CustomerImportance DataFrame not loaded, cannot calculate weight percentiles. PatId1 will use fixed fallback.\")\n",
    "\n",
    "\n",
    "# --- Read Transaction Stream (cleanSource DISABLED) ---\n",
    "transactions_stream_df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(transaction_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(INPUT_S3_PATH)\n",
    "    # .option(\"cleanSource\", \"archive\")  # <<<< KEPT DISABLED TO AVOID S3 ARCHIVER ISSUES\n",
    "    # .option(\"sourceArchiveDir\", ARCHIVE_S3_PATH) \n",
    "    \n",
    "print(\"Transaction stream reader defined (cleanSource is DISABLED for this run).\")\n",
    "\n",
    "\n",
    "def get_ist_time():\n",
    "    return datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "detections_buffer = [] \n",
    "DETECTION_BATCH_SIZE = 50\n",
    "\n",
    "# --- foreachBatch function ---\n",
    "def process_batch(batch_df, epoch_id):\n",
    "    global detections_buffer\n",
    "    y_start_time_ist = get_ist_time()\n",
    "    print(f\"\\n--- Processing Batch {epoch_id} starting at {y_start_time_ist} (IST) ---\")\n",
    "\n",
    "    if batch_df.isEmpty():\n",
    "        print(f\"Batch {epoch_id}: Empty. Skipping.\")\n",
    "        return\n",
    "\n",
    "    batch_df.persist()\n",
    "    batch_row_count = batch_df.count() # Action to materialize batch_df\n",
    "    print(f\"Batch {epoch_id} - Initial Row Count: {batch_row_count}\")\n",
    "    if batch_row_count == 0 : # Double check after count\n",
    "        print(f\"Batch {epoch_id}: Confirmed empty after count. Skipping actual processing.\")\n",
    "        batch_df.unpersist()\n",
    "        return\n",
    "\n",
    "    # ---- 1. UPDATE POSTGRESQL WITH BATCH AGGREGATES ----\n",
    "    \n",
    "    # == Upsert Merchant Transaction Summary ==\n",
    "    target_table_mts = \"merchant_transaction_summary\"\n",
    "    temp_table_mts = f\"temp_mts_updates_batch_{epoch_id}\" \n",
    "    \n",
    "    batch_merchant_summary_df = batch_df.groupBy(\"merchant\") \\\n",
    "        .agg(count(\"*\").alias(\"current_batch_tx_count\")) \\\n",
    "        .withColumn(\"current_batch_ts\", current_timestamp()) \\\n",
    "        .withColumnRenamed(\"merchant\", \"merchant_id_src\")\n",
    "\n",
    "    batch_merchant_summary_df.write.mode(\"overwrite\").jdbc(url=PG_JDBC_URL, table=temp_table_mts, properties=PG_PROPERTIES)\n",
    "    print(f\"Batch {epoch_id} - Wrote to {temp_table_mts}\")\n",
    "\n",
    "    conn_mts = None; stmt_mts = None\n",
    "    try:\n",
    "        conn_mts = spark.sparkContext._gateway.jvm.java.sql.DriverManager.getConnection(PG_JDBC_URL, PG_USER, PG_PASSWORD)\n",
    "        conn_mts.setAutoCommit(True)\n",
    "        stmt_sql_mts = f\"\"\"\n",
    "        INSERT INTO {target_table_mts} (merchant_id, total_transactions, last_updated)\n",
    "        SELECT source.merchant_id_src, source.current_batch_tx_count, source.current_batch_ts FROM {temp_table_mts} AS source\n",
    "        ON CONFLICT (merchant_id) DO UPDATE SET\n",
    "            total_transactions = {target_table_mts}.total_transactions + EXCLUDED.total_transactions,\n",
    "            last_updated = EXCLUDED.last_updated;\"\"\"\n",
    "        stmt_mts = conn_mts.createStatement(); rows_affected_mts = stmt_mts.executeUpdate(stmt_sql_mts)\n",
    "        print(f\"Batch {epoch_id} - Upserted {target_table_mts}. Rows: {rows_affected_mts}\")\n",
    "    except Exception as e: print_jdbc_error(e, target_table_mts, epoch_id)\n",
    "    finally: close_jdbc_resources(stmt_mts, conn_mts)\n",
    "\n",
    "    # == Upsert Customer-Merchant Summary ==\n",
    "    target_table_cms = \"customer_merchant_summary\"; temp_table_cms = f\"temp_cms_updates_batch_{epoch_id}\"\n",
    "    batch_customer_merchant_summary_df = batch_df.groupBy(\"customer\", \"merchant\").agg(count(\"*\").alias(\"c_b_tx_c\"), spark_sum(\"amount\").alias(\"c_b_s_a\")).withColumn(\"c_b_ts\", current_timestamp()).withColumnRenamed(\"customer\", \"c_id_src\").withColumnRenamed(\"merchant\", \"m_id_src\")\n",
    "    batch_customer_merchant_summary_df.write.mode(\"overwrite\").jdbc(url=PG_JDBC_URL, table=temp_table_cms, properties=PG_PROPERTIES)\n",
    "    print(f\"Batch {epoch_id} - Wrote to {temp_table_cms}\")\n",
    "    conn_cms = None; stmt_cms = None\n",
    "    try:\n",
    "        conn_cms = spark.sparkContext._gateway.jvm.java.sql.DriverManager.getConnection(PG_JDBC_URL, PG_USER, PG_PASSWORD); conn_cms.setAutoCommit(True)\n",
    "        stmt_sql_cms = f\"\"\"\n",
    "        INSERT INTO {target_table_cms} (customer_id, merchant_id, transaction_count, total_amount_sum, last_updated)\n",
    "        SELECT source.c_id_src, source.m_id_src, source.c_b_tx_c, source.c_b_s_a, source.c_b_ts FROM {temp_table_cms} AS source\n",
    "        ON CONFLICT (customer_id, merchant_id) DO UPDATE SET\n",
    "            transaction_count = {target_table_cms}.transaction_count + EXCLUDED.transaction_count,\n",
    "            total_amount_sum = COALESCE({target_table_cms}.total_amount_sum, 0.0) + COALESCE(EXCLUDED.total_amount_sum, 0.0),\n",
    "            last_updated = EXCLUDED.last_updated;\"\"\"\n",
    "        stmt_cms = conn_cms.createStatement(); rows_affected_cms = stmt_cms.executeUpdate(stmt_sql_cms)\n",
    "        print(f\"Batch {epoch_id} - Upserted {target_table_cms}. Rows: {rows_affected_cms}\")\n",
    "    except Exception as e: print_jdbc_error(e, target_table_cms, epoch_id)\n",
    "    finally: close_jdbc_resources(stmt_cms, conn_cms)\n",
    "\n",
    "    # == Upsert Merchant Gender Summary ==\n",
    "    target_table_mgs = \"merchant_gender_summary\"; temp_table_mgs = f\"temp_mgs_updates_batch_{epoch_id}\"\n",
    "    batch_merchant_gender_pivot_df = batch_df.groupBy(\"merchant\").pivot(\"gender\", [\"M\", \"F\"]).agg(count(\"*\")).fillna(0)\n",
    "    if 'M' not in batch_merchant_gender_pivot_df.columns: batch_merchant_gender_pivot_df = batch_merchant_gender_pivot_df.withColumn('M', lit(0))\n",
    "    if 'F' not in batch_merchant_gender_pivot_df.columns: batch_merchant_gender_pivot_df = batch_merchant_gender_pivot_df.withColumn('F', lit(0))\n",
    "    batch_merchant_gender_summary_df = batch_merchant_gender_pivot_df.select(col(\"merchant\").alias(\"m_id_src\"), col(\"M\").alias(\"c_b_m_c\"), col(\"F\").alias(\"c_b_f_c\")).withColumn(\"c_b_ts\", current_timestamp())\n",
    "    batch_merchant_gender_summary_df.write.mode(\"overwrite\").jdbc(url=PG_JDBC_URL, table=temp_table_mgs, properties=PG_PROPERTIES)\n",
    "    print(f\"Batch {epoch_id} - Wrote to {temp_table_mgs}\")\n",
    "    conn_mgs = None; stmt_mgs = None\n",
    "    try:\n",
    "        conn_mgs = spark.sparkContext._gateway.jvm.java.sql.DriverManager.getConnection(PG_JDBC_URL, PG_USER, PG_PASSWORD); conn_mgs.setAutoCommit(True)\n",
    "        stmt_sql_mgs = f\"\"\"\n",
    "        INSERT INTO {target_table_mgs} (merchant_id, male_transaction_count, female_transaction_count, last_updated)\n",
    "        SELECT source.m_id_src, source.c_b_m_c, source.c_b_f_c, source.c_b_ts FROM {temp_table_mgs} AS source\n",
    "        ON CONFLICT (merchant_id) DO UPDATE SET\n",
    "            male_transaction_count = {target_table_mgs}.male_transaction_count + EXCLUDED.male_transaction_count,\n",
    "            female_transaction_count = {target_table_mgs}.female_transaction_count + EXCLUDED.female_transaction_count,\n",
    "            last_updated = EXCLUDED.last_updated;\"\"\"\n",
    "        stmt_mgs = conn_mgs.createStatement(); rows_affected_mgs = stmt_mgs.executeUpdate(stmt_sql_mgs)\n",
    "        print(f\"Batch {epoch_id} - Upserted {target_table_mgs}. Rows: {rows_affected_mgs}\")\n",
    "    except Exception as e: print_jdbc_error(e, target_table_mgs, epoch_id)\n",
    "    finally: close_jdbc_resources(stmt_mgs, conn_mgs)\n",
    "\n",
    "    # ---- 2. READ FULL STATE FROM POSTGRESQL ----\n",
    "    try:\n",
    "        pg_merchant_summary_df = spark.read.jdbc(url=PG_JDBC_URL, table=\"merchant_transaction_summary\", properties=PG_PROPERTIES)\n",
    "        pg_customer_merchant_summary_df = spark.read.jdbc(url=PG_JDBC_URL, table=\"customer_merchant_summary\", properties=PG_PROPERTIES)\n",
    "        pg_merchant_gender_summary_df = spark.read.jdbc(url=PG_JDBC_URL, table=\"merchant_gender_summary\", properties=PG_PROPERTIES)\n",
    "        print(f\"Batch {epoch_id} - Successfully read summary tables from PostgreSQL.\")\n",
    "    except Exception as e_read_pg:\n",
    "        print(f\"ERROR Batch {epoch_id} - reading summary tables: {e_read_pg}. Creating empty fallback.\")\n",
    "        pg_merchant_summary_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema_merchant_summary)\n",
    "        pg_customer_merchant_summary_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema_customer_merchant_summary)\n",
    "        pg_merchant_gender_summary_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema_merchant_gender_summary)\n",
    "\n",
    "    # ---- 3. ENRICH BATCH DATA ----\n",
    "    enriched_df = batch_df.join(customer_importance_df, (batch_df.customer == customer_importance_df.Source) & (batch_df.merchant == customer_importance_df.Target) & (batch_df.category == customer_importance_df.typeTrans), \"left_outer\")\n",
    "\n",
    "    # ---- 4. DETECT PATTERNS ----\n",
    "    # TEST Thresholds\n",
    "    PATID1_MERCHANT_TX_THRESHOLD = 5; PATID1_CUSTOMER_TX_THRESHOLD_PER_MERCHANT = 2\n",
    "    PATID2_MIN_TRANSACTIONS = 3; PATID2_AVG_AMOUNT_THRESHOLD = 23.0\n",
    "    PATID3_MIN_FEMALE_TRANSACTIONS = 2 \n",
    "\n",
    "    enriched_df_persisted = enriched_df.persist() # Persist for reuse in PatId1 low weight check\n",
    "\n",
    "    active_merchants_df = pg_merchant_summary_df.filter(col(\"total_transactions\") > PATID1_MERCHANT_TX_THRESHOLD).select(col(\"merchant_id\").alias(\"upg_mid\"))\n",
    "    high_tx_cust_df = pg_customer_merchant_summary_df.filter(col(\"transaction_count\") > PATID1_CUSTOMER_TX_THRESHOLD_PER_MERCHANT).select(col(\"customer_id\").alias(\"upg_cid\"), col(\"merchant_id\").alias(\"upg_mid_cust\"))\n",
    "        \n",
    "    if weight_percentiles_df is not None and not weight_percentiles_df.rdd.isEmpty():\n",
    "        low_weight_df = enriched_df_persisted.join(weight_percentiles_df, (enriched_df_persisted.merchant == weight_percentiles_df.merchant_key) & (enriched_df_persisted.category == weight_percentiles_df.category_key),\"inner\").filter(col(\"Weight\") < col(\"p1_weight\")).select(\"customer\", \"merchant\").distinct().withColumnRenamed(\"customer\", \"lw_cid\").withColumnRenamed(\"merchant\", \"lw_mid\")\n",
    "    else: \n",
    "        low_weight_df = enriched_df_persisted.filter(col(\"Weight\").isNotNull() & (col(\"Weight\") < 2.0)).select(\"customer\", \"merchant\").distinct().withColumnRenamed(\"customer\", \"lw_cid\").withColumnRenamed(\"merchant\", \"lw_mid\")\n",
    "\n",
    "    patid1_detections = active_merchants_df.join(high_tx_cust_df, active_merchants_df.upg_mid == high_tx_cust_df.upg_mid_cust, \"inner\").join(low_weight_df,(col(\"upg_mid\") == col(\"lw_mid\")) & (col(\"upg_cid\") == col(\"lw_cid\")),\"inner\").select(lit(y_start_time_ist).alias(\"YStartTime\"),lit(get_ist_time()).alias(\"DetectionTime\"),lit(\"PatId1\").alias(\"PatternId\"),lit(\"UPGRADE\").alias(\"ActionType\"),col(\"upg_cid\").alias(\"CustomerName\"),col(\"upg_mid\").alias(\"MerchantId\")).distinct()\n",
    "    \n",
    "    enriched_df_persisted.unpersist()\n",
    "\n",
    "    patid2_detections = pg_customer_merchant_summary_df.withColumn(\"avg_tx_val\", coalesce(col(\"total_amount_sum\"), lit(0.0)) / coalesce(col(\"transaction_count\"), lit(1.0))).filter((col(\"transaction_count\") >= PATID2_MIN_TRANSACTIONS) & (col(\"avg_tx_val\") < PATID2_AVG_AMOUNT_THRESHOLD)).select(lit(y_start_time_ist).alias(\"YStartTime\"),lit(get_ist_time()).alias(\"DetectionTime\"),lit(\"PatId2\").alias(\"PatternId\"),lit(\"CHILD\").alias(\"ActionType\"),col(\"customer_id\").alias(\"CustomerName\"),col(\"merchant_id\").alias(\"MerchantId\"))\n",
    "    patid3_detections = pg_merchant_gender_summary_df.filter((col(\"female_transaction_count\") < col(\"male_transaction_count\")) & (col(\"female_transaction_count\") > PATID3_MIN_FEMALE_TRANSACTIONS)).select(lit(y_start_time_ist).alias(\"YStartTime\"),lit(get_ist_time()).alias(\"DetectionTime\"),lit(\"PatId3\").alias(\"PatternId\"),lit(\"DEI-NEEDED\").alias(\"ActionType\"),lit(\"\").alias(\"CustomerName\"),col(\"merchant_id\").alias(\"MerchantId\"))\n",
    "\n",
    "    # ---- 5. COMBINE AND WRITE DETECTIONS ----\n",
    "    all_detections_list = [patid1_detections, patid2_detections, patid3_detections]\n",
    "    final_detection_columns = [\"YStartTime\", \"DetectionTime\", \"PatternId\", \"ActionType\", \"CustomerName\", \"MerchantId\"]\n",
    "    \n",
    "    # Initialize an empty DataFrame with the target schema for unioning\n",
    "    # This ensures a base schema if all individual detection DFs are empty\n",
    "    unioned_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), output_df_schema)\n",
    "\n",
    "    for df_to_union in all_detections_list:\n",
    "        if df_to_union is not None and not df_to_union.rdd.isEmpty():\n",
    "            # Ensure the df_to_union also conforms to the final_detection_columns or schema before union\n",
    "            # Our construction of patidX_detections should make them conformant.\n",
    "             unioned_df = unioned_df.unionByName(df_to_union.select(final_detection_columns).fillna(\"\")) # Select and fillna before union\n",
    "\n",
    "    all_detections_df_final = unioned_df # Now unioned_df has all detections or is empty with correct schema\n",
    "            \n",
    "    collected_detections = all_detections_df_final.collect()\n",
    "    \n",
    "    batch_df.unpersist() # enriched_df was unpersisted earlier\n",
    "\n",
    "    if collected_detections:\n",
    "        print(f\"Batch {epoch_id} - Collected {len(collected_detections)} detections.\")\n",
    "        detections_buffer.extend(collected_detections) \n",
    "        while len(detections_buffer) >= DETECTION_BATCH_SIZE:\n",
    "            to_write_list = detections_buffer[:DETECTION_BATCH_SIZE]\n",
    "            detections_buffer = detections_buffer[DETECTION_BATCH_SIZE:]\n",
    "            if to_write_list:\n",
    "                output_df = spark.createDataFrame(to_write_list, schema=output_df_schema) \n",
    "                output_subdir_name = f\"detections_batch_{epoch_id}_{str(uuid.uuid4())[:8]}\"\n",
    "                full_output_path = OUTPUT_S3_PATH + output_subdir_name\n",
    "                print(f\"Batch {epoch_id} - Writing {len(to_write_list)} detections to {full_output_path}\")\n",
    "                output_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(full_output_path)\n",
    "    else:\n",
    "        print(f\"Batch {epoch_id} - No detections in this batch.\")\n",
    "        \n",
    "    print(f\"--- Finished processing Batch {epoch_id} ---\")\n",
    "\n",
    "# Helper for JDBC error printing\n",
    "def print_jdbc_error(e, table_name, epoch_id):\n",
    "    print(f\"FATAL ERROR upserting {table_name} in Batch {epoch_id}: {e}\")\n",
    "    if hasattr(e, 'java_exception') and e.java_exception is not None:\n",
    "        java_ex = e.java_exception\n",
    "        print(f\"  JAVA EXCEPTION Type: {type(java_ex)}\")\n",
    "        print(f\"  JAVA EXCEPTION Msg: {java_ex.getMessage()}\")\n",
    "        if hasattr(java_ex, 'getSQLState'): print(f\"  JAVA EXCEPTION SQLState: {java_ex.getSQLState()}\")\n",
    "        if hasattr(java_ex, 'getErrorCode'): print(f\"  JAVA EXCEPTION ErrorCode: {java_ex.getErrorCode()}\")\n",
    "        java_ex.printStackTrace()\n",
    "\n",
    "# Helper for closing JDBC resources\n",
    "def close_jdbc_resources(stmt, conn):\n",
    "    if stmt is not None:\n",
    "        try: stmt.close()\n",
    "        except Exception as e_cls_stmt: print(f\"WARN: Error closing statement: {e_cls_stmt}\")\n",
    "    if conn is not None:\n",
    "        try: conn.close()\n",
    "        except Exception as e_cls_conn: print(f\"WARN: Error closing connection: {e_cls_conn}\")\n",
    "\n",
    "# --- Start the Streaming Query ---\n",
    "print(\"Starting FINAL Corrected streaming query (with explicit INSERT ON CONFLICT and cleanSource DISABLED)...\")\n",
    "query = transactions_stream_df.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_S3_PATH) \\\n",
    "    .trigger(processingTime='30 seconds') \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Streaming query '{query.name}' (id: {query.id}) started.\")\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism Y",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
